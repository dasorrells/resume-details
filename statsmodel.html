<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Statistical Test Framework</title>

  <!-- Google Analytics Tag -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-2XDS0NXE42"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() {
      dataLayer.push(arguments);
    }
    gtag("js", new Date());
    gtag("config", "G-2XDS0NXE42");
  </script>

  <style>
    body {
      font-family: Arial, sans-serif;
      line-height: 1.6;
      margin: 20px;
      color: #333;
    }

    h1,
    h2,
    h3 {
      color: #444;
    }

    ul {
      margin: 10px 0;
      padding-left: 20px;
    }

    ul li {
      margin-bottom: 10px;
    }

    .highlight {
      font-weight: bold;
      color: #0056b3;
    }

    .center {
      text-align: center;
      margin: 20px 0;
    }

    p.gray {
      color: #666;
      font-style: italic;
      font-weight: bold;
    }

    hr {
      border: 0;
      border-top: 1px solid #ccc;
    }
  </style>
</head>

<body>
  <h1 id="stat-framework-summary">Statistical Test Framework</h1>

  <p class="gray">
    “Designed and implemented a statistical framework combining power analysis, clustering, and Difference-in-Differences to accelerate test readouts, reduce variance, and detect practical effects across hundreds of centers.”
  </p>

  <h2>1. Measurement Design</h2>
  <p>
    Power analysis is used prior to experimentation to estimate the minimum number of 28-day periods needed to detect a given lift in return rate with high statistical confidence. Assumptions include expected standard deviation, target lift size, and control/test group ratios.
  </p>

  <h2>2. Clustering and Matching</h2>
  <p>
    Centers are grouped using k-means clustering based on historical KPIs, geographic proximity, and maturity. This reduces within-group variance and improves match quality for test/control assignments.
  </p>

  <h2>3. Pre/Post Analysis with Difference-in-Differences</h2>
  <p>
    Metrics are averaged across pre- and post-periods (e.g., 6 months before and after launch). Difference-in-Differences estimation isolates causal effects by comparing changes in test groups vs matched controls.
  </p>

  <h2>4. Statistical Significance and Effect Size</h2>
  <p>
    Statistical rigor is ensured using p-values, 95% confidence intervals, and effect size calculations (Cohen’s d) to validate both the presence and magnitude of change.
  </p>

  <div class="center">
    <hr />
  </div>

  <h2>Bottom Line</h2>
  <p>
    This process creates a defensible measurement strategy grounded in power analysis, clean experimental design, and rapid statistical readouts—supporting fast iteration and confident decision-making at scale.
  </p>
</body>

</html>
