<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Test vs Control Experimentation Framework</title>

  <script async src="https://www.googletagmanager.com/gtag/js?id=G-2XDS0NXE42"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() {
      dataLayer.push(arguments);
    }
    gtag("js", new Date());
    gtag("config", "G-2XDS0NXE42");
  </script>

  <style>
    body {
      font-family: Arial, sans-serif;
      line-height: 1.6;
      margin: 20px;
      color: #333;
    }

    h1, h2, h3 {
      color: #444;
    }

    ul {
      margin: 10px 0;
      padding-left: 20px;
    }

    ul li {
      margin-bottom: 10px;
    }

    .highlight {
      font-weight: bold;
      color: #0056b3;
    }

    .center {
      text-align: center;
      margin: 20px 0;
    }

    p.gray {
      color: #666;
      font-style: italic;
      font-weight: bold;
    }

    hr {
      border: 0;
      border-top: 1px solid #ccc;
    }
  </style>
</head>

<body>
  <h1 id="framework-summary">Statistical Test vs Control Experimentation Framework</h1>

  <p class="gray">
    “Built a scalable statistical framework combining power analysis, clustering, and Difference-in-Differences to analyze change, quantify effect size, and accelerate time to significance across all test vs control experiments.”
  </p>

  <h2>1. Purpose</h2>
  <p>
    This framework was developed to support consistent, defensible measurement across all types of controlled experiments—pricing, product, retention, or operational. It is designed to enable repeatable analysis of behavioral change, isolate treatment effects, and generate rapid statistical readouts.
  </p>

  <h2>2. Measurement Design & Power Analysis</h2>
  <ul>
    <li>Leverages <code>statsmodels</code> in Python to conduct pre-test power analysis</li>
    <li>Estimates sample size and timeframes needed based on assumed variance, test/control size, and target lift</li>
    <li>Allows teams to evaluate feasibility before launching an experiment</li>
  </ul>

  <h2>3. Clustering & Control Matching</h2>
  <p>
    To minimize bias and improve precision, k-means clustering is used to match centers based on maturity, historical KPIs, and geography. This reduces variance, strengthens comparisons, and ensures balanced control group design.
  </p>

  <h2>4. Difference-in-Differences Pre/Post Analysis</h2>
  <p>
    Pre- and post-period metrics are averaged at the center level, and Difference-in-Differences is used to isolate causal impact. This approach helps control for time trends, seasonality, and structural differences between groups.
  </p>

  <h2>5. Statistical Significance & Effect Interpretation</h2>
  <ul>
    <li>Reports p-values and 95% confidence intervals to validate statistical significance</li>
    <li>Calculates Cohen’s d to measure practical impact</li>
    <li>Identifies which tests are meaningful enough to influence strategy</li>
  </ul>

  <h2>6. Modular, Reusable Design</h2>
  <p>
    The framework is built with reusability in mind using <code>pandas</code>, <code>numpy</code>, and <code>statsmodels</code>. Analysts can plug in different test types, timeframes, and KPIs without reengineering the logic—supporting scalable experimentation across business domains.
  </p>

  <div class="center">
    <hr />
  </div>

  <h2>Bottom Line</h2>
  <p>
    This experiment measurement system provides a robust and repeatable approach to evaluating change. By combining statistical rigor with automation and flexibility, it supports confident decision-making, rapid iteration, and data-driven innovation at scale.
  </p>
</body>

</html>
